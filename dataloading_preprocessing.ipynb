{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bBQSzXzNf5E"
   },
   "source": [
    "# import packages and connect to google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NexBOCdejMC",
    "outputId": "c69bd301-0db8-43aa-db45-91a6d3d4267f"
   },
   "outputs": [],
   "source": [
    "!pip install mat73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9C-yt38WOxWw",
    "outputId": "6aef7c6c-53df-4cc0-8040-d4464b829cb4"
   },
   "outputs": [],
   "source": [
    "!pip install vitaldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9OCM6H7ZMSJ7",
    "outputId": "35a4c542-35ff-4659-f46e-3876810c6353"
   },
   "outputs": [],
   "source": [
    "# import the right packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import einops as eo # Added\n",
    "import pathlib as pl # Added\n",
    "\n",
    "from mat73 import loadmat\n",
    "\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "import wfdb\n",
    "import ast\n",
    "import pickle\n",
    "import gzip\n",
    "import random\n",
    "import urllib.request\n",
    "import shutil\n",
    "import ast\n",
    "from multiprocessing import Pool, cpu_count\n",
    "# from tqdm import tqdm # Removed redundant import\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import gdown\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import collections  as mc\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import entropy\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.signal import resample\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#transformer:\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from time import time as timer\n",
    "import umap\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Audio\n",
    "import IPython\n",
    "\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm.auto import tqdm # Corrected import to get the callable tqdm function\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import sys\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import glob\n",
    "import vitaldb\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXlVRgLoPYD1"
   },
   "source": [
    "# Load data (do not rerun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BolYDWz0PaKG"
   },
   "source": [
    "https://github.com/pulselabteam/PulseDB\n",
    "\n",
    "https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2022.1090854/full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF8NRCbNVakm"
   },
   "source": [
    "it seems that I have shared the access to the file containing the segmented data in google drive. data are now at this link: https://drive.google.com/drive/folders/1QZ-Z-C9MoLk1S9MovF3Tt6NjAEGbSJI3\n",
    "\n",
    "there is no need to download file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnyNL-caH337"
   },
   "source": [
    "!!!!! for the shortcut: first ask to share the file. then the file will appeared in shared file in Gdrive. then create a shortcut to put it where you want it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtIpPGvB9Yjo",
    "outputId": "badd10e2-695e-4511-e682-5817207421d8"
   },
   "outputs": [],
   "source": [
    "# Navigate to the folder\n",
    "data_dir = '/content/drive/MyDrive/CAS_FinalProject/PulseDB_Vital' # link to the shortcut\n",
    "save_dir = '/content/drive/MyDrive/CAS_FinalProject/CAS_FinalProject_RAW/Vital1'\n",
    "\n",
    "############################################### careful: the data at this link contain only age and sex demographic info\n",
    "\n",
    "# data_dir = '/content/drive/MyDrive/Colab_Notebooks/CAS_FinalProject/PulseDB_Vital'# Colab_Notebook changed to remove the space; filed moved to /content/drive/MyDrive/CAS_FinalProject/CAS_FinalProject_RAW/PulseDB_Vital\n",
    "# save_dir = '/content/drive/MyDrive/Colab_Notebooks/CAS_FinalProject/Vital' ##### Note that there is Colab_Notebook in the file path/ data moved to /content/drive/MyDrive/CAS_FinalProject/CAS_FinalProject_RAW/Vital\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Progress tracking file\n",
    "progress_file = os.path.join(save_dir, 'processing_progress.json')\n",
    "\n",
    "# List all .mat files\n",
    "mat_files = sorted(glob.glob(os.path.join(data_dir, '*.mat')))\n",
    "print(f\"Found {len(mat_files)} .mat files\\n\")\n",
    "\n",
    "\n",
    "def Build_Dataset(file_path, FieldName='Subj_Wins'):\n",
    "    \"\"\"Load PulseDB segment file and extract signals and demographics\"\"\"\n",
    "    Data = loadmat(file_path)\n",
    "    SubjData = Data[FieldName]\n",
    "\n",
    "    # Access and squeeze signals\n",
    "    ECG_Record_F = np.squeeze(np.array(SubjData['ECG_Record_F']))\n",
    "    PPG_Record_F = np.squeeze(np.array(SubjData['PPG_Record_F']))\n",
    "    T = np.squeeze(np.array(SubjData['T']))\n",
    "\n",
    "    # Ensure 2D\n",
    "    if ECG_Record_F.ndim == 1:\n",
    "        ECG_Record_F = ECG_Record_F.reshape(1, -1)\n",
    "        PPG_Record_F = PPG_Record_F.reshape(1, -1)\n",
    "        T = T.reshape(1, -1)\n",
    "\n",
    "    # Stack signals\n",
    "    n_segments = ECG_Record_F.shape[0]\n",
    "    signal_length = ECG_Record_F.shape[1]\n",
    "    Signals = np.zeros((n_segments, 3, signal_length))\n",
    "    Signals[:, 0, :] = ECG_Record_F\n",
    "    Signals[:, 1, :] = PPG_Record_F\n",
    "    Signals[:, 2, :] = T\n",
    "\n",
    "    # Demographics\n",
    "    Age = np.array(SubjData['Age']).flatten()\n",
    "    Gender = np.array(SubjData['Gender']).flatten()\n",
    "\n",
    "    if Gender.dtype.kind in ['U', 'S']:\n",
    "        Gender = np.array([1.0 if 'M' in str(g).upper() else 0.0 for g in Gender])\n",
    "    else:\n",
    "        Gender = (Gender == 'M').astype(float)\n",
    "\n",
    "    Height = np.array(SubjData['Height']).flatten()\n",
    "    Weight = np.array(SubjData['Weight']).flatten()\n",
    "\n",
    "    if Age.size == 1 and n_segments > 1:\n",
    "        Age = np.repeat(Age, n_segments)\n",
    "        Gender = np.repeat(Gender, n_segments)\n",
    "        Height = np.repeat(Height, n_segments)\n",
    "        Weight = np.repeat(Weight, n_segments)\n",
    "\n",
    "    Demographics = np.stack((Age, Gender, Height, Weight), axis=1)\n",
    "\n",
    "    return Signals, Demographics\n",
    "\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load processing progress from file\"\"\"\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            progress = json.load(f)\n",
    "        print(f\"Resuming from batch {progress['last_batch'] + 1}\")\n",
    "        print(f\"Already processed: {progress['processed_files']} files\")\n",
    "        return progress\n",
    "    else:\n",
    "        return {'last_batch': -1, 'processed_files': 0, 'processed_filenames': []}\n",
    "\n",
    "\n",
    "def save_progress(batch_num, processed_count, processed_filenames):\n",
    "    \"\"\"Save processing progress to file\"\"\"\n",
    "    progress = {\n",
    "        'last_batch': batch_num,\n",
    "        'processed_files': processed_count,\n",
    "        'processed_filenames': processed_filenames\n",
    "    }\n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump(progress, f)\n",
    "\n",
    "\n",
    "def save_batch(batch_num, signals, demographics, subject_ids):\n",
    "    \"\"\"Save a batch of processed data\"\"\"\n",
    "    batch_path = os.path.join(save_dir, f'PulseDB_Vital_Batch_{batch_num:04d}.npz')\n",
    "    np.savez_compressed(batch_path,\n",
    "                        Signals=signals,\n",
    "                        Demographics=demographics,\n",
    "                        SubjectIDs=subject_ids)\n",
    "    print(f\"✅ Saved batch {batch_num} to: {batch_path}\")\n",
    "    return batch_path\n",
    "\n",
    "\n",
    "def load_all_batches():\n",
    "    \"\"\"Load all previously saved batches and combine them\"\"\"\n",
    "    batch_files = sorted(glob.glob(os.path.join(save_dir, 'PulseDB_Vital_Batch_*.npz')))\n",
    "\n",
    "    if not batch_files:\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"\\nLoading {len(batch_files)} existing batches...\")\n",
    "    all_signals = []\n",
    "    all_demographics = []\n",
    "    all_subject_ids = []\n",
    "\n",
    "    for batch_file in batch_files:\n",
    "        data = np.load(batch_file)\n",
    "        all_signals.append(data['Signals'])\n",
    "        all_demographics.append(data['Demographics'])\n",
    "        all_subject_ids.append(data['SubjectIDs']) ############################ changed\n",
    "\n",
    "    combined_signals = np.concatenate(all_signals, axis=0)\n",
    "    combined_demographics = np.concatenate(all_demographics, axis=0)\n",
    "    combined_subject_ids = np.concatenate(all_subject_ids, axis=0) ############################## changed\n",
    "\n",
    "    print(f\"Loaded {len(batch_files)} batches with {combined_signals.shape[0]} total segments\")\n",
    "\n",
    "    return combined_signals, combined_demographics, combined_subject_ids\n",
    "\n",
    "\n",
    "# Load progress\n",
    "progress = load_progress()\n",
    "start_idx = progress['processed_files']\n",
    "processed_filenames = set(progress['processed_filenames'])\n",
    "\n",
    "# Skip already processed files\n",
    "remaining_files = [f for f in mat_files if os.path.basename(f) not in processed_filenames]\n",
    "print(f\"Remaining files to process: {len(remaining_files)}\")\n",
    "\n",
    "# Process files in batches of 50\n",
    "BATCH_SIZE = 50\n",
    "batch_num = progress['last_batch'] + 1\n",
    "\n",
    "batch_signals = []\n",
    "batch_demographics = []\n",
    "batch_subject_ids = []\n",
    "total_processed = start_idx\n",
    "\n",
    "print(f\"\\nStarting processing from file {start_idx + 1}...\")\n",
    "\n",
    "for idx, mat_file in enumerate(remaining_files):\n",
    "    try:\n",
    "        Signals, Demographics = Build_Dataset(mat_file)\n",
    "\n",
    "        batch_signals.append(Signals)\n",
    "        batch_demographics.append(Demographics)\n",
    "\n",
    "        # Extract subject ID from filename\n",
    "        subject_id = os.path.splitext(os.path.basename(mat_file))[0]\n",
    "        batch_subject_ids.extend([subject_id] * Signals.shape[0])\n",
    "\n",
    "        processed_filenames.add(os.path.basename(mat_file))\n",
    "        total_processed += 1\n",
    "\n",
    "        # Save every 50 subjects\n",
    "        if len(batch_signals) >= BATCH_SIZE:\n",
    "            print(f\"\\nSaving batch {batch_num} ({len(batch_signals)} subjects)...\")\n",
    "\n",
    "            # Concatenate batch data\n",
    "            batch_signals_concat = np.concatenate(batch_signals, axis=0)\n",
    "            batch_demographics_concat = np.concatenate(batch_demographics, axis=0)\n",
    "            batch_subject_ids_array = np.array(batch_subject_ids)\n",
    "\n",
    "            # Save batch\n",
    "            save_batch(batch_num, batch_signals_concat, batch_demographics_concat, batch_subject_ids_array)\n",
    "\n",
    "            # Update progress\n",
    "            save_progress(batch_num, total_processed, list(processed_filenames))\n",
    "\n",
    "            # Reset batch lists\n",
    "            batch_signals = []\n",
    "            batch_demographics = []\n",
    "            batch_subject_ids = []\n",
    "            batch_num += 1\n",
    "\n",
    "        # Progress update\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(remaining_files)} remaining files (Total: {total_processed}/{len(mat_files)})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(mat_file)}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "# Save remaining subjects (less than 50)\n",
    "if len(batch_signals) > 0:\n",
    "    print(f\"\\nSaving final batch {batch_num} ({len(batch_signals)} subjects)...\")\n",
    "    batch_signals_concat = np.concatenate(batch_signals, axis=0)\n",
    "    batch_demographics_concat = np.concatenate(batch_demographics, axis=0)\n",
    "    batch_subject_ids_array = np.array(batch_subject_ids)\n",
    "\n",
    "    save_batch(batch_num, batch_signals_concat, batch_demographics_concat, batch_subject_ids_array)\n",
    "    save_progress(batch_num, total_processed, list(processed_filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psUeOmZjpaD6",
    "outputId": "dfef6f08-e596-4542-8c1d-8aae01d9b913"
   },
   "outputs": [],
   "source": [
    "# Path to your npz file\n",
    "file_path = '/content/drive/MyDrive/CAS_FinalProject/CAS_FinalProject_RAW/Vital1/PulseDB_Vital_Batch_0000.npz'\n",
    "\n",
    "# Load the npz file\n",
    "data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "if 'Demographics' in data.files:\n",
    "    demographics = data['Demographics']\n",
    "    print(\"Demographics shape:\", demographics.shape)\n",
    "    print(\"Demographics dtype:\", demographics.dtype)\n",
    "\n",
    "\n",
    "    print(\"\\nFirst 5 rows of Demographics:\")\n",
    "    print(demographics[:5])\n",
    "else:\n",
    "    print(\"Demographics key not found in the file.\")\n",
    "\n",
    "if 'SubjectIDs' in data.files:\n",
    "    subject_ids = data['SubjectIDs']\n",
    "    print(f\"Total subject IDs: {len(subject_ids)}\")\n",
    "\n",
    "    unique_subject_ids = np.unique(subject_ids)\n",
    "    print(f\"Unique subject IDs ({len(unique_subject_ids)}):\")\n",
    "    print(unique_subject_ids)\n",
    "\n",
    "    print(\"\\nFirst 20 subject IDs:\")\n",
    "    print(subject_ids[:20]) # there are multiple segment per subject... make sense\n",
    "else:\n",
    "    print(\"SubjectIDs key not found in the file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-Ei92ovUXO9"
   },
   "source": [
    "# add BMI and BMI category data in the demographic file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQSUZhX8Vb6t",
    "outputId": "6916dbe2-58b1-4103-8f8a-150cd33f1267"
   },
   "outputs": [],
   "source": [
    "# Paths - IMPORTANT: Use the original batch directory\n",
    "save_dir = '/content/drive/MyDrive/CAS_FinalProject/CAS_FinalProject_RAW/Vital1'\n",
    "output_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_With_BMI1' ########################## make sure this is BMI1\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def calculate_bmi(height_cm, weight_kg):\n",
    "    \"\"\"Calculate BMI from height (cm) and weight (kg)\"\"\"\n",
    "    if np.isnan(height_cm) or np.isnan(weight_kg) or height_cm <= 0:\n",
    "        return np.nan\n",
    "    height_m = height_cm / 100.0\n",
    "    bmi = weight_kg / (height_m ** 2)\n",
    "    return bmi\n",
    "\n",
    "def get_bmi_category(bmi):\n",
    "    \"\"\"Categorize BMI\"\"\"\n",
    "    if np.isnan(bmi):\n",
    "        return 'Unknown'\n",
    "    elif bmi < 18.5:\n",
    "        return 'Underweight'\n",
    "    elif bmi < 25.0:\n",
    "        return 'Normal Weight'\n",
    "    elif bmi < 30.0:\n",
    "        return 'Overweight'\n",
    "    else:\n",
    "        return 'Obese'\n",
    "\n",
    "# Process each batch\n",
    "batch_files = sorted(glob.glob(os.path.join(save_dir, 'PulseDB_Vital_Batch_*.npz')))\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Adding BMI and BMI_Category to Demographics \")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "total_segments = 0\n",
    "valid_bmi_count = 0\n",
    "bmi_category_counts = {'Underweight': 0, 'Normal Weight': 0, 'Overweight': 0, 'Obese': 0, 'Unknown': 0}\n",
    "\n",
    "for idx, batch_file in enumerate(batch_files):\n",
    "    batch_num = int(os.path.basename(batch_file).split('_')[-1].replace('.npz', ''))\n",
    "\n",
    "    print(f\"Processing Batch {batch_num:04d}...\", end=' ')\n",
    "\n",
    "    # Load batch - ONLY LOAD, DON'T PROCESS SIGNALS\n",
    "    data = np.load(batch_file, allow_pickle=True)\n",
    "    signals = data['Signals']                    # Keep as-is\n",
    "    demographics = data['Demographics']          # Modify only this\n",
    "    subject_ids = data['SubjectIDs']            # Keep as-is\n",
    "\n",
    "    n_segments = demographics.shape[0]\n",
    "    total_segments += n_segments\n",
    "\n",
    "    # Calculate BMI and BMI_Category for each segment\n",
    "    bmi_values = np.zeros(n_segments)\n",
    "    bmi_categories = np.empty(n_segments, dtype=object)\n",
    "    bmi_category_codes = np.zeros(n_segments, dtype=int)\n",
    "\n",
    "    for seg_idx in range(n_segments):\n",
    "        # Extract height and weight from original demographics\n",
    "        # Original format: [Age, Gender, Height(cm), Weight(kg)]\n",
    "        height_cm = demographics[seg_idx, 2]\n",
    "        weight_kg = demographics[seg_idx, 3]\n",
    "\n",
    "        # Calculate BMI\n",
    "        bmi = calculate_bmi(height_cm, weight_kg)\n",
    "        bmi_values[seg_idx] = bmi\n",
    "\n",
    "        # Get BMI category\n",
    "        bmi_category = get_bmi_category(bmi)\n",
    "        bmi_categories[seg_idx] = bmi_category\n",
    "\n",
    "        # Encode BMI_Category as numeric\n",
    "        bmi_category_codes[seg_idx] = (\n",
    "            0 if bmi_category == 'Unknown' else\n",
    "            1 if bmi_category == 'Underweight' else\n",
    "            2 if bmi_category == 'Normal Weight' else\n",
    "            3 if bmi_category == 'Overweight' else\n",
    "            4  # Obese\n",
    "        )\n",
    "\n",
    "        # Update statistics\n",
    "        if not np.isnan(bmi):\n",
    "            valid_bmi_count += 1\n",
    "        bmi_category_counts[bmi_category] += 1\n",
    "\n",
    "    # CREATE NEW DEMOGRAPHICS WITH BMI ADDED\n",
    "    # New format: [Age, Gender, Height, Weight, BMI, BMI_Category_Code]\n",
    "    demographics_with_bmi = np.column_stack((\n",
    "        demographics,              # Original 4 columns\n",
    "        bmi_values,               # Add BMI\n",
    "        bmi_category_codes        # Add BMI_Category_Code\n",
    "    ))\n",
    "\n",
    "    # Save batch with ONLY demographics modified\n",
    "    output_path = os.path.join(output_dir, f'PulseDB_Vital_Batch_{batch_num:04d}.npz')\n",
    "    np.savez_compressed(output_path,\n",
    "                        Signals=signals,              # ✅ UNCHANGED\n",
    "                        Demographics=demographics_with_bmi,  # ✅ MODIFIED\n",
    "                        SubjectIDs=subject_ids,       # ✅ UNCHANGED\n",
    "                        BMI_Categories=bmi_categories)  # ✅ NEW (optional, for reference)\n",
    "\n",
    "    print(f\"✅ Saved ({n_segments} segments)\")\n",
    "\n",
    "    # Clean up\n",
    "    data.close()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✅ BMI Addition Complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total segments processed: {total_segments:,}\")\n",
    "print(f\"Valid BMI values: {valid_bmi_count:,} ({100*valid_bmi_count/total_segments:.1f}%)\")\n",
    "\n",
    "print(f\"\\nBMI Category Distribution:\")\n",
    "for category, count in bmi_category_counts.items():\n",
    "    percentage = 100 * count / total_segments if total_segments > 0 else 0\n",
    "    print(f\"  {category:20s}: {count:>6,} ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nOutput batches saved to: {output_dir}\")\n",
    "print(f\"Demographics format: [Age, Gender, Height(cm), Weight(kg), BMI, BMI_Category_Code]\")\n",
    "print(f\"BMI_Category_Code: 1=Underweight, 2=Normal, 3=Overweight, 4=Obese, 0=Unknown\")\n",
    "\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dVcbiPj1V4s3",
    "outputId": "2b19bac3-b35e-4f4f-eb2d-e8a30b6f5057"
   },
   "outputs": [],
   "source": [
    "# Path to batches with BMI\n",
    "output_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_With_BMI1'\n",
    "\n",
    "# Load all batches and collect BMI data\n",
    "batch_files = sorted(glob.glob(os.path.join(output_dir, 'PulseDB_Vital_Batch_*.npz')))\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Collecting BMI Data from {len(batch_files)} batches...\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "all_bmi_data = []\n",
    "\n",
    "for batch_idx, batch_file in enumerate(batch_files):\n",
    "    batch_num = int(os.path.basename(batch_file).split('_')[-1].replace('.npz', ''))\n",
    "\n",
    "    # Load batch\n",
    "    data = np.load(batch_file, allow_pickle=True)\n",
    "    demographics = data['Demographics']\n",
    "    bmi_categories = data['BMI_Categories']\n",
    "    subject_ids = data['SubjectIDs']\n",
    "\n",
    "    # Extract BMI data (column 4) and category (column 5)\n",
    "    for seg_idx in range(len(demographics)):\n",
    "        bmi = demographics[seg_idx, 4]\n",
    "        bmi_code = int(demographics[seg_idx, 5])\n",
    "        bmi_category = bmi_categories[seg_idx]\n",
    "        subject_id = subject_ids[seg_idx]\n",
    "\n",
    "        all_bmi_data.append({\n",
    "            'BMI': bmi,\n",
    "            'BMI_Code': bmi_code,\n",
    "            'BMI_Category': bmi_category,\n",
    "            'Subject_ID': subject_id,\n",
    "            'Batch': batch_num\n",
    "        })\n",
    "\n",
    "    data.close()\n",
    "\n",
    "    if (batch_idx + 1) % 3 == 0:\n",
    "        print(f\"Processed {batch_idx + 1}/{len(batch_files)} batches...\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_bmi = pd.DataFrame(all_bmi_data)\n",
    "\n",
    "# Get unique subjects (one row per subject)\n",
    "df_unique = df_bmi.drop_duplicates(subset=['Subject_ID']).copy()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BMI DISTRIBUTION STATISTICS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total segments: {len(df_bmi):,}\")\n",
    "print(f\"Unique subjects: {len(df_unique):,}\")\n",
    "\n",
    "print(f\"\\nBMI Statistics (All Segments):\")\n",
    "print(f\"  Mean: {df_bmi['BMI'].mean():.2f}\")\n",
    "print(f\"  Std:  {df_bmi['BMI'].std():.2f}\")\n",
    "print(f\"  Min:  {df_bmi['BMI'].min():.2f}\")\n",
    "print(f\"  Max:  {df_bmi['BMI'].max():.2f}\")\n",
    "print(f\"  Median: {df_bmi['BMI'].median():.2f}\")\n",
    "\n",
    "print(f\"\\nBMI Statistics (Unique Subjects):\")\n",
    "print(f\"  Mean: {df_unique['BMI'].mean():.2f}\")\n",
    "print(f\"  Std:  {df_unique['BMI'].std():.2f}\")\n",
    "print(f\"  Min:  {df_unique['BMI'].min():.2f}\")\n",
    "print(f\"  Max:  {df_unique['BMI'].max():.2f}\")\n",
    "print(f\"  Median: {df_unique['BMI'].median():.2f}\")\n",
    "\n",
    "print(f\"\\nBMI Category Distribution (All Segments):\")\n",
    "category_counts = df_bmi['BMI_Category'].value_counts()\n",
    "for category, count in category_counts.items():\n",
    "    percentage = 100 * count / len(df_bmi)\n",
    "    print(f\"  {category:20s}: {count:>8,} ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nBMI Category Distribution (Unique Subjects):\")\n",
    "category_counts_unique = df_unique['BMI_Category'].value_counts()\n",
    "for category, count in category_counts_unique.items():\n",
    "    percentage = 100 * count / len(df_unique)\n",
    "    print(f\"  {category:20s}: {count:>4,} ({percentage:>5.1f}%)\")\n",
    "\n",
    "# ===== VISUALIZATIONS =====\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "fig.suptitle('PulseDB_Vital Dataset - BMI Distribution Analysis',\n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# ===== ROW 1: Histograms =====\n",
    "\n",
    "# 1. BMI Histogram (All Segments)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.hist(df_bmi['BMI'].dropna(), bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(df_bmi['BMI'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df_bmi[\"BMI\"].mean():.1f}')\n",
    "ax1.axvline(df_bmi['BMI'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df_bmi[\"BMI\"].median():.1f}')\n",
    "ax1.set_xlabel('BMI', fontsize=11)\n",
    "ax1.set_ylabel('Frequency (All Segments)', fontsize=11)\n",
    "ax1.set_title('BMI Distribution - All Segments', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. BMI Histogram (Unique Subjects)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.hist(df_unique['BMI'].dropna(), bins=40, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(df_unique['BMI'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df_unique[\"BMI\"].mean():.1f}')\n",
    "ax2.axvline(df_unique['BMI'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df_unique[\"BMI\"].median():.1f}')\n",
    "ax2.set_xlabel('BMI', fontsize=11)\n",
    "ax2.set_ylabel('Frequency (Unique Subjects)', fontsize=11)\n",
    "ax2.set_title('BMI Distribution - Unique Subjects', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. BMI with Category Regions\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.hist(df_unique['BMI'].dropna(), bins=40, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "# Add category region lines\n",
    "ax3.axvline(18.5, color='blue', linestyle='-', linewidth=2, alpha=0.7, label='Underweight')\n",
    "ax3.axvline(25.0, color='green', linestyle='-', linewidth=2, alpha=0.7, label='Normal')\n",
    "ax3.axvline(30.0, color='orange', linestyle='-', linewidth=2, alpha=0.7, label='Overweight')\n",
    "ax3.axvspan(30, df_unique['BMI'].max() + 5, alpha=0.2, color='red', label='Obese')\n",
    "ax3.set_xlabel('BMI', fontsize=11)\n",
    "ax3.set_ylabel('Frequency', fontsize=11)\n",
    "ax3.set_title('BMI Categories Regions', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== ROW 2: Category Distributions =====\n",
    "\n",
    "# 4. BMI Categories - Bar Chart (All Segments)\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "category_order = ['Underweight', 'Normal Weight', 'Overweight', 'Obese', 'Unknown']\n",
    "counts_all = [df_bmi[df_bmi['BMI_Category'] == cat].shape[0] for cat in category_order]\n",
    "colors_cat = ['lightblue', 'lightgreen', 'orange', 'red', 'gray']\n",
    "bars = ax4.bar(category_order, counts_all, color=colors_cat, edgecolor='black', alpha=0.7)\n",
    "ax4.set_ylabel('Count (All Segments)', fontsize=11)\n",
    "ax4.set_title('BMI Categories - All Segments', fontsize=12, fontweight='bold')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, counts_all):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(count):,}', ha='center', va='bottom', fontsize=9)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. BMI Categories - Bar Chart (Unique Subjects)\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "counts_unique = [df_unique[df_unique['BMI_Category'] == cat].shape[0] for cat in category_order]\n",
    "bars = ax5.bar(category_order, counts_unique, color=colors_cat, edgecolor='black', alpha=0.7)\n",
    "ax5.set_ylabel('Count (Unique Subjects)', fontsize=11)\n",
    "ax5.set_title('BMI Categories - Unique Subjects', fontsize=12, fontweight='bold')\n",
    "ax5.tick_params(axis='x', rotation=45)\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, counts_unique):\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(count)}', ha='center', va='bottom', fontsize=9)\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6. BMI Categories - Pie Chart (Unique Subjects)\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "# Filter out Unknown for cleaner pie chart\n",
    "df_unique_known = df_unique[df_unique['BMI_Category'] != 'Unknown']\n",
    "pie_counts = df_unique_known['BMI_Category'].value_counts()\n",
    "pie_data = [pie_counts.get(cat, 0) for cat in ['Underweight', 'Normal Weight', 'Overweight', 'Obese']]\n",
    "wedges, texts, autotexts = ax6.pie(pie_data,\n",
    "                                     labels=['Underweight', 'Normal Weight', 'Overweight', 'Obese'],\n",
    "                                     colors=['lightblue', 'lightgreen', 'orange', 'red'],\n",
    "                                     autopct='%1.1f%%',\n",
    "                                     startangle=90)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('black')\n",
    "    autotext.set_fontweight('bold')\n",
    "ax6.set_title('BMI Categories Distribution\\n(Unique Subjects, Known BMI)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# ===== ROW 3: Box Plots and Statistics =====\n",
    "\n",
    "# 7. Box Plot by Category (All Segments)\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "data_by_cat = [df_bmi[df_bmi['BMI_Category'] == cat]['BMI'].dropna().values for cat in category_order if cat != 'Unknown']\n",
    "bp = ax7.boxplot(data_by_cat, labels=['Underweight', 'Normal', 'Overweight', 'Obese'],\n",
    "                 patch_artist=True, widths=0.6)\n",
    "for patch, color in zip(bp['boxes'], colors_cat[:4]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax7.set_ylabel('BMI', fontsize=11)\n",
    "ax7.set_title('BMI Distribution by Category\\n(All Segments)', fontsize=12, fontweight='bold')\n",
    "ax7.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 8. Box Plot by Category (Unique Subjects)\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "data_by_cat_unique = [df_unique[df_unique['BMI_Category'] == cat]['BMI'].dropna().values for cat in category_order if cat != 'Unknown']\n",
    "bp = ax8.boxplot(data_by_cat_unique, labels=['Underweight', 'Normal', 'Overweight', 'Obese'],\n",
    "                 patch_artist=True, widths=0.6)\n",
    "for patch, color in zip(bp['boxes'], colors_cat[:4]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax8.set_ylabel('BMI', fontsize=11)\n",
    "ax8.set_title('BMI Distribution by Category\\n(Unique Subjects)', fontsize=12, fontweight='bold')\n",
    "ax8.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 9. Summary Statistics Table\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "ax9.axis('off')\n",
    "\n",
    "# Create summary text\n",
    "summary_text = f\"\"\"\n",
    "DATASET SUMMARY\n",
    "\n",
    "Total Segments: {len(df_bmi):,}\n",
    "Unique Subjects: {len(df_unique):,}\n",
    "\n",
    "BMI STATISTICS (Unique Subjects):\n",
    "  Mean: {df_unique['BMI'].mean():.2f}\n",
    "  Std: {df_unique['BMI'].std():.2f}\n",
    "  Min: {df_unique['BMI'].min():.2f}\n",
    "  Max: {df_unique['BMI'].max():.2f}\n",
    "\n",
    "CATEGORY BREAKDOWN:\n",
    "  Underweight: {counts_unique[0]:>4} ({100*counts_unique[0]/len(df_unique):>5.1f}%)\n",
    "  Normal: {counts_unique[1]:>4} ({100*counts_unique[1]/len(df_unique):>5.1f}%)\n",
    "  Overweight: {counts_unique[2]:>4} ({100*counts_unique[2]/len(df_unique):>5.1f}%)\n",
    "  Obese: {counts_unique[3]:>4} ({100*counts_unique[3]/len(df_unique):>5.1f}%)\n",
    "\"\"\"\n",
    "\n",
    "ax9.text(0.1, 0.5, summary_text, fontsize=10, verticalalignment='center',\n",
    "        fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.savefig('bmi_distribution_analysis.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Visualization saved as 'bmi_distribution_analysis.png'\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3JdMQksyCtT",
    "outputId": "85080148-bdb7-4b47-9e34-5bd4253c9d2e"
   },
   "outputs": [],
   "source": [
    "# check data\n",
    "# Correct path\n",
    "save_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_With_BMI1'###################### check BMI1\n",
    "\n",
    "def load_batch(batch_num):\n",
    "    \"\"\"Load a specific batch file\"\"\"\n",
    "    batch_path = os.path.join(save_dir, f'PulseDB_Vital_Batch_{batch_num:04d}.npz')\n",
    "    if os.path.exists(batch_path):\n",
    "        data = np.load(batch_path, allow_pickle=True)\n",
    "        return data['Signals'], data['Demographics'], data['SubjectIDs']\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def iterate_batches():\n",
    "    \"\"\"Generator to iterate through batches without loading all into memory\"\"\"\n",
    "    batch_files = sorted(glob.glob(os.path.join(save_dir, 'PulseDB_Vital_Batch_*.npz')))\n",
    "\n",
    "    for batch_file in batch_files:\n",
    "        data = np.load(batch_file, allow_pickle=True)\n",
    "        yield data['Signals'], data['Demographics'], data['SubjectIDs']\n",
    "        data.close()  # Free memory\n",
    "\n",
    "\n",
    "def get_dataset_info():\n",
    "    \"\"\"Get info about the complete dataset without loading it all\"\"\"\n",
    "    batch_files = sorted(glob.glob(os.path.join(save_dir, 'PulseDB_Vital_Batch_*.npz')))\n",
    "\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"PulseDB_Vital Dataset Information\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total batch files: {len(batch_files)}\")\n",
    "\n",
    "    total_segments = 0\n",
    "    total_subjects = set()\n",
    "\n",
    "    # Collect demographics for statistics\n",
    "    all_ages = []\n",
    "    all_genders = []\n",
    "    all_heights = []\n",
    "    all_weights = []\n",
    "\n",
    "    for idx, batch_file in enumerate(batch_files):\n",
    "        file_size_mb = os.path.getsize(batch_file) / (1024**2)\n",
    "\n",
    "        data = np.load(batch_file, allow_pickle=True)\n",
    "        n_segments = data['Signals'].shape[0]\n",
    "        signal_length = data['Signals'].shape[2]\n",
    "        subjects = set(data['SubjectIDs'])\n",
    "\n",
    "        # Collect demographics\n",
    "        all_ages.extend(data['Demographics'][:, 0])\n",
    "        all_genders.extend(data['Demographics'][:, 1])\n",
    "        all_heights.extend(data['Demographics'][:, 2])\n",
    "        all_weights.extend(data['Demographics'][:, 3])\n",
    "\n",
    "        total_segments += n_segments\n",
    "        total_subjects.update(subjects)\n",
    "\n",
    "        print(f\"  Batch {idx:02d}: {n_segments:>6,} segments, {len(subjects):>3} subjects, {file_size_mb:>6.1f} MB\")\n",
    "\n",
    "        data.close()\n",
    "\n",
    "    # Convert to arrays for statistics\n",
    "    all_ages = np.array(all_ages)\n",
    "    all_genders = np.array(all_genders)\n",
    "    all_heights = np.array(all_heights)\n",
    "    all_weights = np.array(all_weights)\n",
    "\n",
    "    total_size_mb = sum([os.path.getsize(f) / (1024**2) for f in batch_files])\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total segments: {total_segments:,}\")\n",
    "    print(f\"Total unique subjects: {len(total_subjects)}\")\n",
    "    print(f\"Samples per segment: {signal_length} (10 seconds @ {signal_length/10:.0f} Hz)\")\n",
    "    print(f\"Total dataset size: {total_size_mb:.1f} MB ({total_size_mb/1024:.2f} GB)\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Demographics Summary\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Age:\")\n",
    "    print(f\"  Mean: {all_ages.mean():.1f} years\")\n",
    "    print(f\"  Std:  {all_ages.std():.1f} years\")\n",
    "    print(f\"  Range: {all_ages.min():.0f} - {all_ages.max():.0f} years\")\n",
    "\n",
    "    print(f\"\\nGender:\")\n",
    "    n_male = (all_genders == 1).sum()\n",
    "    n_female = (all_genders == 0).sum()\n",
    "    print(f\"  Male:   {n_male:,} ({100*n_male/len(all_genders):.1f}%)\")\n",
    "    print(f\"  Female: {n_female:,} ({100*n_female/len(all_genders):.1f}%)\")\n",
    "\n",
    "    print(f\"\\nHeight:\")\n",
    "    print(f\"  Mean: {np.nanmean(all_heights):.1f} cm\")\n",
    "    print(f\"  Std:  {np.nanstd(all_heights):.1f} cm\")\n",
    "\n",
    "    print(f\"\\nWeight:\")\n",
    "    print(f\"  Mean: {np.nanmean(all_weights):.1f} kg\")\n",
    "    print(f\"  Std:  {np.nanstd(all_weights):.1f} kg\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return {\n",
    "        'n_batches': len(batch_files),\n",
    "        'total_segments': total_segments,\n",
    "        'n_subjects': len(total_subjects),\n",
    "        'signal_length': signal_length,\n",
    "        'batch_files': batch_files\n",
    "    }\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "print(\"\\nAnalyzing your PulseDB_Vital dataset...\\n\")\n",
    "info = get_dataset_info()\n",
    "\n",
    "# Example: Load and use first batch\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Example: Working with Batch 0\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "signals, demographics, subject_ids = load_batch(0)\n",
    "\n",
    "if signals is not None:\n",
    "    print(f\"Loaded Batch 0:\")\n",
    "    print(f\"  Signals shape: {signals.shape}\")\n",
    "    print(f\"    - signals[:, 0, :] = ECG\")\n",
    "    print(f\"    - signals[:, 1, :] = PPG\")\n",
    "    print(f\"    - signals[:, 2, :] = Time\")\n",
    "    print(f\"  Demographics shape: {demographics.shape}\")\n",
    "    print(f\"    - demographics[:, 0] = Age\")\n",
    "    print(f\"    - demographics[:, 1] = Gender (1=M, 0=F)\")\n",
    "    print(f\"    - demographics[:, 2] = Height (cm)\")\n",
    "    print(f\"    - demographics[:, 3] = Weight (kg)\")\n",
    "    print(f\"  Number of subjects: {len(np.unique(subject_ids))}\")\n",
    "\n",
    "    # Extract ECG and PPG\n",
    "    ecg_batch0 = signals[:, 0, :]\n",
    "    ppg_batch0 = signals[:, 1, :]\n",
    "\n",
    "    print(f\"\\n  ECG shape: {ecg_batch0.shape}\")\n",
    "    print(f\"  PPG shape: {ppg_batch0.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzaBeHR-1MRW"
   },
   "source": [
    "# Create a new subset containing 589 subjects with 150 from Underweight and Normal, 211 from overweight and 89 obeses (that is the max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuchpGI_ZMyt",
    "outputId": "1a45ef3f-074c-4e3e-c0a9-e7c7b270f1dc"
   },
   "outputs": [],
   "source": [
    "save_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_With_BMI1'\n",
    "output_dir_balanced = '/content/drive/MyDrive/CAS_FinalProject/Vital_With_BMI1_balanced'\n",
    "os.makedirs(output_dir_balanced, exist_ok=True)\n",
    "\n",
    "# NEW: Adjusted balanced counts per BMI category (total 600 subjects)\n",
    "subjects_per_category = {\n",
    "    'Underweight': 150,\n",
    "    'Normal Weight': 150,\n",
    "    'Overweight': 211,\n",
    "    'Obese': 89\n",
    "}\n",
    "\n",
    "target_bmi_categories = ['Underweight', 'Normal Weight', 'Overweight', 'Obese']\n",
    "\n",
    "# Track subjects selected for each category\n",
    "selected_subjects_per_category = {cat: set() for cat in target_bmi_categories}\n",
    "\n",
    "# Map from subject to all its indices and BMI categories\n",
    "subject_info = defaultdict(lambda: {'indices': [], 'bmi_cats': []})\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 1: Aggregate subject info across batches\n",
    "# ---------------------------------------------------\n",
    "batch_files = sorted(glob.glob(os.path.join(save_dir, 'PulseDB_Vital_Batch_*.npz')))\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Aggregating subject info and BMI categories from batches\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for batch_file in batch_files:\n",
    "    data = np.load(batch_file, allow_pickle=True)\n",
    "    demographics = data['Demographics']\n",
    "    bmi_categories = data['BMI_Categories']\n",
    "    subject_ids = data['SubjectIDs']\n",
    "    n_segments = len(subject_ids)\n",
    "\n",
    "    for i in range(n_segments):\n",
    "        subj = subject_ids[i]\n",
    "        subject_info[subj]['indices'].append((batch_file, i))\n",
    "        subject_info[subj]['bmi_cats'].append(bmi_categories[i])\n",
    "\n",
    "    data.close()\n",
    "\n",
    "print(f\"Total unique subjects found: {len(subject_info)}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 2: Determine each subject's BMI category by mode\n",
    "# ---------------------------------------------------\n",
    "subject_bmi_category = {}\n",
    "\n",
    "for subj, info in subject_info.items():\n",
    "    bmi_cats = info['bmi_cats']\n",
    "    # Get the most common BMI category for this subject\n",
    "    counts = defaultdict(int)\n",
    "    for cat in bmi_cats:\n",
    "        counts[cat] += 1\n",
    "    mode_cat = max(counts, key=counts.get)\n",
    "    subject_bmi_category[subj] = mode_cat\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 3: Select subjects for balanced dataset (NEW COUNTS)\n",
    "# ---------------------------------------------------\n",
    "total_selected = 0\n",
    "for cat in target_bmi_categories:\n",
    "    # Collect all subjects with this BMI category\n",
    "    subjects_in_cat = [s for s,c in subject_bmi_category.items() if c == cat]\n",
    "    needed = subjects_per_category[cat]\n",
    "\n",
    "    print(f\"{cat}: found {len(subjects_in_cat)} subjects, selecting {needed}\")\n",
    "\n",
    "    if len(subjects_in_cat) < needed:\n",
    "        print(f\"  WARNING: Only {len(subjects_in_cat)} available, using all\")\n",
    "        sampled = subjects_in_cat\n",
    "    else:\n",
    "        # Randomly sample without replacement\n",
    "        sampled = np.random.choice(subjects_in_cat, size=needed, replace=False)\n",
    "\n",
    "    selected_subjects_per_category[cat].update(sampled)\n",
    "    total_selected += len(sampled)\n",
    "\n",
    "selected_subjects_all = set()\n",
    "for cat in target_bmi_categories:\n",
    "    selected_subjects_all.update(selected_subjects_per_category[cat])\n",
    "\n",
    "print(f\"Total selected subjects for balanced dataset: {total_selected}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 4: Extract segments for selected subjects and save in batches of 50 subjects\n",
    "# ---------------------------------------------------\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Extracting segments and saving balanced dataset in batches of 10 subjects\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Prepare container for accumulating batch data before saving\n",
    "batch_size_subjects = 50\n",
    "current_batch_subjects = []\n",
    "current_signals = []\n",
    "current_demographics = []\n",
    "current_subject_ids = []\n",
    "current_bmi_categories = []\n",
    "\n",
    "def save_batch(batch_idx):\n",
    "    out_path = os.path.join(output_dir_balanced, f'Balanced_PulseDB_Vital_Batch_{batch_idx:04d}.npz')\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        Signals=np.array(current_signals),\n",
    "        Demographics=np.array(current_demographics),\n",
    "        SubjectIDs=np.array(current_subject_ids),\n",
    "        BMI_Categories=np.array(current_bmi_categories)\n",
    "    )\n",
    "    print(f\"Saved batch {batch_idx:04d}: {len(current_signals)} segments, {len(set(current_subject_ids))} subjects\")\n",
    "\n",
    "batch_idx = 1\n",
    "\n",
    "for cat in target_bmi_categories:\n",
    "    for subj in list(selected_subjects_per_category[cat]):\n",
    "        subj_indices = subject_info[subj]['indices']\n",
    "        # Group indices by batch file for efficiency\n",
    "        batch_file_to_indices = defaultdict(list)\n",
    "        for fpath, idx_seg in subj_indices:\n",
    "            batch_file_to_indices[fpath].append(idx_seg)\n",
    "\n",
    "        for fpath, indices in batch_file_to_indices.items():\n",
    "            data = np.load(fpath, allow_pickle=True)\n",
    "            signals = data['Signals'][indices]\n",
    "            demographics = data['Demographics'][indices]\n",
    "            subject_ids = data['SubjectIDs'][indices]\n",
    "            bmi_cats = data['BMI_Categories'][indices]\n",
    "            data.close()\n",
    "\n",
    "            # Append to current batch containers\n",
    "            current_signals.extend(signals)\n",
    "            current_demographics.extend(demographics)\n",
    "            current_subject_ids.extend(subject_ids)\n",
    "            current_bmi_categories.extend(bmi_cats)\n",
    "\n",
    "        current_batch_subjects.append(subj)\n",
    "\n",
    "        # Save batch if accumulated subjects reach batch_size_subjects\n",
    "        if len(current_batch_subjects) == batch_size_subjects:\n",
    "            save_batch(batch_idx)\n",
    "            batch_idx += 1\n",
    "            current_batch_subjects.clear()\n",
    "            current_signals.clear()\n",
    "            current_demographics.clear()\n",
    "            current_subject_ids.clear()\n",
    "            current_bmi_categories.clear()\n",
    "\n",
    "# Save remaining if any\n",
    "if current_batch_subjects:\n",
    "    save_batch(batch_idx)\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Balanced dataset creation complete!\")\n",
    "print(f\"Final counts: 150 Underweight + 150 Normal + 211 Overweight + 89 Obese = {total_selected} total subjects\")\n",
    "print(f\"Data saved under: {output_dir_balanced}\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oj8jMrAhd8h"
   },
   "source": [
    "# create a subset containing Normal only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFTN_ad-p5YM",
    "outputId": "a526f3e8-dc10-4073-f8ec-782b3a1f9408"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_With_BMI1'\n",
    "output_dir_balanced = '/content/drive/MyDrive/CAS_FinalProject/Vital_With_BMI1_Normal'\n",
    "os.makedirs(output_dir_balanced, exist_ok=True)\n",
    "\n",
    "# ONLY Normal Weight patients: 600 subjects\n",
    "subjects_per_category = {'Normal Weight': 600}\n",
    "target_bmi_categories = ['Normal Weight']  # Only Normal Weight\n",
    "\n",
    "# Track subjects selected\n",
    "selected_subjects_per_category = {cat: set() for cat in target_bmi_categories}\n",
    "\n",
    "# Map from subject to all its indices and BMI categories\n",
    "subject_info = defaultdict(lambda: {'indices': [], 'bmi_cats': []})\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 1: Aggregate subject info across batches\n",
    "# ---------------------------------------------------\n",
    "batch_files = sorted(glob.glob(os.path.join(save_dir, 'PulseDB_Vital_Batch_*.npz')))\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Aggregating subject info - Normal Weight patients only\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for batch_file in batch_files:\n",
    "    data = np.load(batch_file, allow_pickle=True)\n",
    "    demographics = data['Demographics']\n",
    "    bmi_categories = data['BMI_Categories']\n",
    "    subject_ids = data['SubjectIDs']\n",
    "    n_segments = len(subject_ids)\n",
    "\n",
    "    for i in range(n_segments):\n",
    "        subj = subject_ids[i]\n",
    "        subject_info[subj]['indices'].append((batch_file, i))\n",
    "        subject_info[subj]['bmi_cats'].append(bmi_categories[i])\n",
    "\n",
    "    data.close()\n",
    "\n",
    "print(f\"Total unique subjects found: {len(subject_info)}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 2: Determine each subject's BMI category by mode\n",
    "# ---------------------------------------------------\n",
    "subject_bmi_category = {}\n",
    "\n",
    "for subj, info in subject_info.items():\n",
    "    bmi_cats = info['bmi_cats']\n",
    "    # Get the most common BMI category for this subject\n",
    "    counts = defaultdict(int)\n",
    "    for cat in bmi_cats:\n",
    "        counts[cat] += 1\n",
    "    mode_cat = max(counts, key=counts.get)\n",
    "    subject_bmi_category[subj] = mode_cat\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 3: Select 600 Normal Weight subjects\n",
    "# ---------------------------------------------------\n",
    "total_selected = 0\n",
    "for cat in target_bmi_categories:\n",
    "    # Collect all Normal Weight subjects\n",
    "    subjects_in_cat = [s for s,c in subject_bmi_category.items() if c == cat]\n",
    "    needed = subjects_per_category[cat]\n",
    "\n",
    "    print(f\"{cat}: found {len(subjects_in_cat)} subjects, selecting {needed}\")\n",
    "\n",
    "    if len(subjects_in_cat) < needed:\n",
    "        print(f\"  WARNING: Only {len(subjects_in_cat)} available, using all\")\n",
    "        sampled = subjects_in_cat\n",
    "    else:\n",
    "        # Randomly sample 600 Normal Weight subjects\n",
    "        sampled = np.random.choice(subjects_in_cat, size=needed, replace=False)\n",
    "\n",
    "    selected_subjects_per_category[cat].update(sampled)\n",
    "    total_selected += len(sampled)\n",
    "\n",
    "print(f\"Total Normal Weight subjects selected: {total_selected}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 4: Extract segments and save in batches of 50 subjects\n",
    "# ---------------------------------------------------\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Extracting Normal Weight segments - batches of 50 subjects\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Prepare container for accumulating batch data\n",
    "batch_size_subjects = 50\n",
    "current_batch_subjects = []\n",
    "current_signals = []\n",
    "current_demographics = []\n",
    "current_subject_ids = []\n",
    "current_bmi_categories = []\n",
    "\n",
    "def save_batch(batch_idx):\n",
    "    out_path = os.path.join(output_dir_balanced, f'Normal_PulseDB_Vital_Batch_{batch_idx:04d}.npz')\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        Signals=np.array(current_signals),\n",
    "        Demographics=np.array(current_demographics),\n",
    "        SubjectIDs=np.array(current_subject_ids),\n",
    "        BMI_Categories=np.array(current_bmi_categories)\n",
    "    )\n",
    "    print(f\"Saved batch {batch_idx:04d}: {len(current_signals)} segments, {len(set(current_subject_ids))} subjects\")\n",
    "\n",
    "batch_idx = 1\n",
    "\n",
    "# Process only Normal Weight selected subjects\n",
    "for subj in list(selected_subjects_per_category['Normal Weight']):\n",
    "    subj_indices = subject_info[subj]['indices']\n",
    "    # Group indices by batch file for efficiency\n",
    "    batch_file_to_indices = defaultdict(list)\n",
    "    for fpath, idx_seg in subj_indices:\n",
    "        batch_file_to_indices[fpath].append(idx_seg)\n",
    "\n",
    "    for fpath, indices in batch_file_to_indices.items():\n",
    "        data = np.load(fpath, allow_pickle=True)\n",
    "        signals = data['Signals'][indices]\n",
    "        demographics = data['Demographics'][indices]\n",
    "        subject_ids = data['SubjectIDs'][indices]\n",
    "        bmi_cats = data['BMI_Categories'][indices]\n",
    "        data.close()\n",
    "\n",
    "        # Append to current batch containers\n",
    "        current_signals.extend(signals)\n",
    "        current_demographics.extend(demographics)\n",
    "        current_subject_ids.extend(subject_ids)\n",
    "        current_bmi_categories.extend(bmi_cats)\n",
    "\n",
    "    current_batch_subjects.append(subj)\n",
    "\n",
    "    # Save batch if accumulated 50 subjects\n",
    "    if len(current_batch_subjects) == batch_size_subjects:\n",
    "        save_batch(batch_idx)\n",
    "        batch_idx += 1\n",
    "        current_batch_subjects.clear()\n",
    "        current_signals.clear()\n",
    "        current_demographics.clear()\n",
    "        current_subject_ids.clear()\n",
    "        current_bmi_categories.clear()\n",
    "\n",
    "# Save remaining subjects\n",
    "if current_batch_subjects:\n",
    "    save_batch(batch_idx)\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Normal Weight dataset creation complete!\")\n",
    "print(f\"Final count: {total_selected} Normal Weight subjects\")\n",
    "print(f\"Data saved under: {output_dir_balanced}\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vD80JfAPQIRg"
   },
   "source": [
    "# Normalize and split each segment into a 4 sec windows with 2 sec overlap (do not rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P65BpT_cAACe",
    "outputId": "c669857e-35c8-4376-b5f6-b6d11334a7b0"
   },
   "outputs": [],
   "source": [
    "# normalize each segment individually per segment\n",
    "\n",
    "save_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_With_BMI1_balanced'################# make sure it is Vital_With_BMI1\n",
    "normalized_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_Normalized1_balanced'############### Vital_Normalized1\n",
    "os.makedirs(normalized_dir, exist_ok=True)\n",
    "\n",
    "def normalize_to_range(signal, min_val=-1, max_val=1):\n",
    "    \"\"\"\n",
    "    Normalize signal to specified range [min_val, max_val]\n",
    "    Default: [-1, 1]\n",
    "    \"\"\"\n",
    "    sig_min = signal.min()\n",
    "    sig_max = signal.max()\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if sig_max - sig_min == 0:\n",
    "        return np.zeros_like(signal)\n",
    "\n",
    "    # Min-Max normalization formula\n",
    "    normalized = (signal - sig_min) / (sig_max - sig_min)  # [0, 1]\n",
    "    normalized = normalized * (max_val - min_val) + min_val  # [min_val, max_val]\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# Process each batch\n",
    "batch_files = sorted(glob.glob(os.path.join(save_dir, 'Balanced_PulseDB_Vital_Batch_*.npz')))\n",
    "\n",
    "print(f\"Normalizing {len(batch_files)} batches...\\n\")\n",
    "\n",
    "for idx, batch_file in enumerate(batch_files):\n",
    "    batch_num = int(os.path.basename(batch_file).split('_')[-1].replace('.npz', ''))\n",
    "\n",
    "    print(f\"Processing Batch {batch_num:04d}...\", end=' ')\n",
    "\n",
    "    # Load batch\n",
    "    data = np.load(batch_file, allow_pickle=True)\n",
    "    signals = data['Signals']\n",
    "    demographics = data['Demographics']\n",
    "    subject_ids = data['SubjectIDs']\n",
    "\n",
    "    # Get original shape\n",
    "    n_segments, n_channels, signal_length = signals.shape\n",
    "\n",
    "    # Create normalized copy\n",
    "    signals_normalized = np.zeros_like(signals)\n",
    "\n",
    "    # Normalize each segment independently\n",
    "    for seg_idx in range(n_segments):\n",
    "        # Normalize ECG (channel 0)\n",
    "        signals_normalized[seg_idx, 0, :] = normalize_to_range(signals[seg_idx, 0, :])\n",
    "\n",
    "        # Normalize PPG (channel 1)\n",
    "        signals_normalized[seg_idx, 1, :] = normalize_to_range(signals[seg_idx, 1, :])\n",
    "\n",
    "        # Keep Time (channel 2) as is (don't normalize time)\n",
    "        signals_normalized[seg_idx, 2, :] = signals[seg_idx, 2, :]\n",
    "\n",
    "    # Save normalized batch\n",
    "    output_path = os.path.join(normalized_dir, f'PulseDB_Vital_Batch_{batch_num:04d}_Normalized.npz')\n",
    "    np.savez_compressed(output_path,\n",
    "                        Signals=signals_normalized,\n",
    "                        Signals_Original=signals,  # Keep original for reference\n",
    "                        Demographics=demographics,\n",
    "                        SubjectIDs=subject_ids)\n",
    "\n",
    "    print(f\"✅ Saved ({n_segments} segments)\")\n",
    "\n",
    "    # Clean up\n",
    "    data.close()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ Normalization complete!\")\n",
    "print(f\"Normalized batches saved to: {normalized_dir}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vayj5eVuyzT5",
    "outputId": "f104d0b6-ff10-4c36-a715-6f7422fbc2d6"
   },
   "outputs": [],
   "source": [
    "## check data: make sure nothing has happen\n",
    "# Correct path\n",
    "save_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_Normalized1_balanced' ##################### make sure it is Vital_Normalized1\n",
    "\n",
    "def load_batch(batch_num):\n",
    "    \"\"\"Load a specific batch file\"\"\"\n",
    "    batch_path = os.path.join(save_dir, f'PulseDB_Vital_Batch_{batch_num:04d}.npz')\n",
    "    if os.path.exists(batch_path):\n",
    "        data = np.load(batch_path, allow_pickle=True)\n",
    "        return data['Signals'], data['Demographics'], data['SubjectIDs']\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def iterate_batches():\n",
    "    \"\"\"Generator to iterate through batches without loading all into memory\"\"\"\n",
    "    batch_files = sorted(glob.glob(os.path.join(save_dir, 'PulseDB_Vital_Batch_*.npz')))\n",
    "\n",
    "    for batch_file in batch_files:\n",
    "        data = np.load(batch_file, allow_pickle=True)\n",
    "        yield data['Signals'], data['Demographics'], data['SubjectIDs']\n",
    "        data.close()  # Free memory\n",
    "\n",
    "\n",
    "def get_dataset_info():\n",
    "    \"\"\"Get info about the complete dataset without loading it all\"\"\"\n",
    "    batch_files = sorted(glob.glob(os.path.join(save_dir, 'PulseDB_Vital_Batch_*.npz')))\n",
    "\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"PulseDB_Vital Dataset Information\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total batch files: {len(batch_files)}\")\n",
    "\n",
    "    total_segments = 0\n",
    "    total_subjects = set()\n",
    "\n",
    "    # Collect demographics for statistics\n",
    "    all_ages = []\n",
    "    all_genders = []\n",
    "    all_heights = []\n",
    "    all_weights = []\n",
    "\n",
    "    for idx, batch_file in enumerate(batch_files):\n",
    "        file_size_mb = os.path.getsize(batch_file) / (1024**2)\n",
    "\n",
    "        data = np.load(batch_file, allow_pickle=True)\n",
    "        n_segments = data['Signals'].shape[0]\n",
    "        signal_length = data['Signals'].shape[2]\n",
    "        subjects = set(data['SubjectIDs'])\n",
    "\n",
    "        # Collect demographics\n",
    "        all_ages.extend(data['Demographics'][:, 0])\n",
    "        all_genders.extend(data['Demographics'][:, 1])\n",
    "        all_heights.extend(data['Demographics'][:, 2])\n",
    "        all_weights.extend(data['Demographics'][:, 3])\n",
    "\n",
    "        total_segments += n_segments\n",
    "        total_subjects.update(subjects)\n",
    "\n",
    "        print(f\"  Batch {idx:02d}: {n_segments:>6,} segments, {len(subjects):>3} subjects, {file_size_mb:>6.1f} MB\")\n",
    "\n",
    "        data.close()\n",
    "\n",
    "    # Convert to arrays for statistics\n",
    "    all_ages = np.array(all_ages)\n",
    "    all_genders = np.array(all_genders)\n",
    "    all_heights = np.array(all_heights)\n",
    "    all_weights = np.array(all_weights)\n",
    "\n",
    "    total_size_mb = sum([os.path.getsize(f) / (1024**2) for f in batch_files])\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total segments: {total_segments:,}\")\n",
    "    print(f\"Total unique subjects: {len(total_subjects)}\")\n",
    "    print(f\"Samples per segment: {signal_length} (10 seconds @ {signal_length/10:.0f} Hz)\")\n",
    "    print(f\"Total dataset size: {total_size_mb:.1f} MB ({total_size_mb/1024:.2f} GB)\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Demographics Summary\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Age:\")\n",
    "    print(f\"  Mean: {all_ages.mean():.1f} years\")\n",
    "    print(f\"  Std:  {all_ages.std():.1f} years\")\n",
    "    print(f\"  Range: {all_ages.min():.0f} - {all_ages.max():.0f} years\")\n",
    "\n",
    "    print(f\"\\nGender:\")\n",
    "    n_male = (all_genders == 1).sum()\n",
    "    n_female = (all_genders == 0).sum()\n",
    "    print(f\"  Male:   {n_male:,} ({100*n_male/len(all_genders):.1f}%)\")\n",
    "    print(f\"  Female: {n_female:,} ({100*n_female/len(all_genders):.1f}%)\")\n",
    "\n",
    "    print(f\"\\nHeight:\")\n",
    "    print(f\"  Mean: {np.nanmean(all_heights):.1f} cm\")\n",
    "    print(f\"  Std:  {np.nanstd(all_heights):.1f} cm\")\n",
    "\n",
    "    print(f\"\\nWeight:\")\n",
    "    print(f\"  Mean: {np.nanmean(all_weights):.1f} kg\")\n",
    "    print(f\"  Std:  {np.nanstd(all_weights):.1f} kg\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return {\n",
    "        'n_batches': len(batch_files),\n",
    "        'total_segments': total_segments,\n",
    "        'n_subjects': len(total_subjects),\n",
    "        'signal_length': signal_length,\n",
    "        'batch_files': batch_files\n",
    "    }\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "print(\"\\nAnalyzing your PulseDB_Vital dataset...\\n\")\n",
    "info = get_dataset_info()\n",
    "\n",
    "# Example: Load and use first batch\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Example: Working with Batch 0\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "signals, demographics, subject_ids = load_batch(0)\n",
    "\n",
    "if signals is not None:\n",
    "    print(f\"Loaded Batch 0:\")\n",
    "    print(f\"  Signals shape: {signals.shape}\")\n",
    "    print(f\"    - signals[:, 0, :] = ECG\")\n",
    "    print(f\"    - signals[:, 1, :] = PPG\")\n",
    "    print(f\"    - signals[:, 2, :] = Time\")\n",
    "    print(f\"  Demographics shape: {demographics.shape}\")\n",
    "    print(f\"    - demographics[:, 0] = Age\")\n",
    "    print(f\"    - demographics[:, 1] = Gender (1=M, 0=F)\")\n",
    "    print(f\"    - demographics[:, 2] = Height (cm)\")\n",
    "    print(f\"    - demographics[:, 3] = Weight (kg)\")\n",
    "    print(f\"  Number of subjects: {len(np.unique(subject_ids))}\")\n",
    "\n",
    "    # Extract ECG and PPG\n",
    "    ecg_batch0 = signals[:, 0, :]\n",
    "    ppg_batch0 = signals[:, 1, :]\n",
    "\n",
    "    print(f\"\\n  ECG shape: {ecg_batch0.shape}\")\n",
    "    print(f\"  PPG shape: {ppg_batch0.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BzEN0zX2-w0"
   },
   "source": [
    " now create windows of 4 sec each with 2 sec overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upl6kT1lAv5o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Da3cryoq3sN0",
    "outputId": "5ec120a5-0905-4093-96e1-efa8d6ebf33f"
   },
   "outputs": [],
   "source": [
    "# NEW: keep subject ID for traceability\n",
    "\n",
    "# Paths\n",
    "normalized_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_Normalized1_balanced' ################### make sure it is 1\n",
    "windowed_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_Windowed1_balanced' ################### make sure it is 1\n",
    "os.makedirs(windowed_dir, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "SEGMENT_DURATION = 10  # seconds\n",
    "WINDOW_DURATION = 4    # seconds\n",
    "OVERLAP = 2            # seconds\n",
    "STRIDE = WINDOW_DURATION - OVERLAP  # 2 seconds\n",
    "SAMPLING_RATE = 125    # Hz (1250 samples / 10 seconds)\n",
    "\n",
    "# Calculate samples\n",
    "samples_per_window = int(WINDOW_DURATION * SAMPLING_RATE)  # 500 samples\n",
    "stride_samples = int(STRIDE * SAMPLING_RATE)                # 250 samples\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"WINDOWING PARAMETERS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Segment duration:     {SEGMENT_DURATION} seconds ({SEGMENT_DURATION * SAMPLING_RATE} samples)\")\n",
    "print(f\"Window duration:      {WINDOW_DURATION} seconds ({samples_per_window} samples)\")\n",
    "print(f\"Overlap:              {OVERLAP} seconds ({OVERLAP * SAMPLING_RATE} samples)\")\n",
    "print(f\"Stride:               {STRIDE} seconds ({stride_samples} samples)\")\n",
    "print(f\"Sampling rate:        {SAMPLING_RATE} Hz\")\n",
    "print(f\"Windows per segment:  {(SEGMENT_DURATION - WINDOW_DURATION) // STRIDE + 1}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "\n",
    "def create_windows(signal, window_size, stride):\n",
    "    \"\"\"\n",
    "    Create overlapping windows from a signal\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal : numpy array of shape (signal_length,)\n",
    "    window_size : int, number of samples per window\n",
    "    stride : int, number of samples to shift between windows\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    windows : numpy array of shape (n_windows, window_size)\n",
    "    \"\"\"\n",
    "    signal_length = len(signal)\n",
    "    n_windows = (signal_length - window_size) // stride + 1\n",
    "\n",
    "    windows = np.zeros((n_windows, window_size))\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        windows[i, :] = signal[start_idx:end_idx]\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "\n",
    "# Process each batch\n",
    "batch_files = sorted(glob.glob(os.path.join(normalized_dir, 'PulseDB_Vital_Batch_*_Normalized.npz')))\n",
    "\n",
    "print(f\"Processing {len(batch_files)} batches...\\n\")\n",
    "\n",
    "total_original_segments = 0\n",
    "total_windows_created = 0\n",
    "\n",
    "for batch_idx, batch_file in enumerate(batch_files):\n",
    "    batch_num = int(os.path.basename(batch_file).split('_')[-2])\n",
    "\n",
    "    print(f\"Processing Batch {batch_num:04d}...\", end=' ')\n",
    "\n",
    "    # Load batch\n",
    "    data = np.load(batch_file, allow_pickle=True)\n",
    "    signals = data['Signals']\n",
    "    demographics = data['Demographics']\n",
    "    subject_ids = data['SubjectIDs']\n",
    "\n",
    "    n_segments = signals.shape[0]\n",
    "    signal_length = signals.shape[2]\n",
    "\n",
    "    # Lists to store windowed data\n",
    "    all_ecg_windows = []\n",
    "    all_ppg_windows = []\n",
    "    all_window_demographics = []\n",
    "    all_window_ids = []\n",
    "    all_window_subject_ids = []\n",
    "\n",
    "    # Process each segment\n",
    "    for seg_idx in range(n_segments):\n",
    "        # Extract ECG and PPG for this segment\n",
    "        ecg_segment = signals[seg_idx, 0, :]\n",
    "        ppg_segment = signals[seg_idx, 1, :]\n",
    "\n",
    "        # Create windows\n",
    "        ecg_windows = create_windows(ecg_segment, samples_per_window, stride_samples)\n",
    "        ppg_windows = create_windows(ppg_segment, samples_per_window, stride_samples)\n",
    "\n",
    "        n_windows = ecg_windows.shape[0]\n",
    "\n",
    "        # Create unique window IDs\n",
    "        # Format: B{batch}_S{segment}_W{window}\n",
    "        # Example: B0000_S00123_W0, B0000_S00123_W1, etc.\n",
    "        for win_idx in range(n_windows):\n",
    "            # Unique window ID\n",
    "            window_id = f\"B{batch_num:04d}_S{seg_idx:05d}_W{win_idx}\"\n",
    "\n",
    "            all_ecg_windows.append(ecg_windows[win_idx, :])\n",
    "            all_ppg_windows.append(ppg_windows[win_idx, :])\n",
    "            all_window_demographics.append(demographics[seg_idx, :])\n",
    "            all_window_ids.append(window_id)\n",
    "            all_window_subject_ids.append(subject_ids[seg_idx])\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_ecg_windows = np.array(all_ecg_windows)\n",
    "    all_ppg_windows = np.array(all_ppg_windows)\n",
    "    all_window_demographics = np.array(all_window_demographics)\n",
    "    all_window_ids = np.array(all_window_ids)\n",
    "    all_window_subject_ids = np.array(all_window_subject_ids)\n",
    "\n",
    "    # Save windowed batch\n",
    "    output_path = os.path.join(windowed_dir, f'PulseDB_Vital_Batch_{batch_num:04d}_Windowed.npz')\n",
    "    np.savez_compressed(output_path,\n",
    "                        ECG_Windows=all_ecg_windows,\n",
    "                        PPG_Windows=all_ppg_windows,\n",
    "                        Demographics=all_window_demographics,\n",
    "                        Window_IDs=all_window_ids,\n",
    "                        SubjectIDs=all_window_subject_ids,\n",
    "                        metadata={\n",
    "                            'window_duration_sec': WINDOW_DURATION,\n",
    "                            'overlap_sec': OVERLAP,\n",
    "                            'stride_sec': STRIDE,\n",
    "                            'sampling_rate_hz': SAMPLING_RATE,\n",
    "                            'samples_per_window': samples_per_window,\n",
    "                            'original_segments': n_segments,\n",
    "                            'total_windows': len(all_window_ids)\n",
    "                        })\n",
    "\n",
    "    total_original_segments += n_segments\n",
    "    total_windows_created += len(all_window_ids)\n",
    "\n",
    "    print(f\"✅ {n_segments:,} segments → {len(all_window_ids):,} windows\")\n",
    "\n",
    "    data.close()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"WINDOWING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total original segments:  {total_original_segments:,}\")\n",
    "print(f\"Total windows created:    {total_windows_created:,}\")\n",
    "print(f\"Windows per segment:      {total_windows_created / total_original_segments:.1f}\")\n",
    "print(f\"Windowed batches saved to: {windowed_dir}\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UGWKAgtDIcu9",
    "outputId": "1ec77012-e46e-4a49-9343-759f60b9ef2f"
   },
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "# Paths\n",
    "normalized_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_Normalized'\n",
    "windowed_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_Windowed'\n",
    "\n",
    "# Load original batch (normalized)\n",
    "original_batch = np.load(os.path.join(normalized_dir, 'PulseDB_Vital_Batch_0000_Normalized.npz'), allow_pickle=True)\n",
    "signals_original = original_batch['Signals']\n",
    "subject_ids_original = original_batch['SubjectIDs']\n",
    "\n",
    "# Load windowed batch\n",
    "windowed_batch = np.load(os.path.join(windowed_dir, 'PulseDB_Vital_Batch_0000_Windowed.npz'), allow_pickle=True)\n",
    "ecg_windows = windowed_batch['ECG_Windows']\n",
    "ppg_windows = windowed_batch['PPG_Windows']\n",
    "window_ids = windowed_batch['Window_IDs']\n",
    "metadata = windowed_batch['metadata'].item()\n",
    "\n",
    "# Parameters\n",
    "SAMPLING_RATE = 125\n",
    "samples_per_window = metadata['samples_per_window']\n",
    "stride_samples = int(metadata['stride_sec'] * SAMPLING_RATE)\n",
    "\n",
    "# Select first segment (segment 0)\n",
    "segment_idx = 0\n",
    "subject_id = subject_ids_original[segment_idx]\n",
    "\n",
    "# Extract original ECG and PPG for this segment\n",
    "ecg_segment = signals_original[segment_idx, 0, :]  # 1250 samples\n",
    "ppg_segment = signals_original[segment_idx, 1, :]  # 1250 samples\n",
    "\n",
    "# Find all windows from this segment\n",
    "segment_window_pattern = f\"B0000_S{segment_idx:05d}_W\"\n",
    "segment_window_indices = [i for i, wid in enumerate(window_ids) if wid.startswith(segment_window_pattern)]\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"VISUALIZATION: Segment {segment_idx} Windowing (ECG & PPG)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Subject ID: {subject_id}\")\n",
    "print(f\"Original segment length: {len(ecg_segment)} samples (10 seconds)\")\n",
    "print(f\"Number of windows: {len(segment_window_indices)}\")\n",
    "print(f\"Window IDs: {[window_ids[i] for i in segment_window_indices]}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Time axes\n",
    "time_original = np.arange(len(ecg_segment)) / SAMPLING_RATE\n",
    "time_window = np.arange(samples_per_window) / SAMPLING_RATE\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "gs = fig.add_gridspec(6, 2, hspace=0.6, wspace=0.3) # Increased hspace\n",
    "\n",
    "# Main title\n",
    "fig.suptitle(f'ECG & PPG Windowing Visualization - Subject {subject_id} (Segment {segment_idx})',\n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# ===== ROW 0: Original Segments =====\n",
    "colors = ['red', 'orange', 'green', 'purple', 'brown']\n",
    "\n",
    "# ECG Original\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(time_original, ecg_segment, 'b-', linewidth=1, label='Original ECG')\n",
    "\n",
    "# Highlight window regions\n",
    "for i, win_idx in enumerate(segment_window_indices):\n",
    "    window_num = int(window_ids[win_idx].split('_W')[-1])\n",
    "    start_sample = window_num * stride_samples\n",
    "    end_sample = start_sample + samples_per_window\n",
    "\n",
    "    start_time = start_sample / SAMPLING_RATE\n",
    "    end_time = end_sample / SAMPLING_RATE\n",
    "\n",
    "    ax1.axvspan(start_time, end_time, alpha=0.2, color=colors[i % len(colors)],\n",
    "                label=f'W{window_num}')\n",
    "\n",
    "ax1.set_xlabel('Time (seconds)', fontsize=11)\n",
    "ax1.set_ylabel('Amplitude', fontsize=11)\n",
    "ax1.set_title('Original ECG Segment (10s) with Window Regions', fontsize=12, fontweight='bold', pad=10) # Increased pad\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(loc='upper right', fontsize=9)\n",
    "ax1.set_xlim([0, 10])\n",
    "ax1.set_ylim([-1.1, 1.1])\n",
    "\n",
    "# PPG Original\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(time_original, ppg_segment, 'r-', linewidth=1, label='Original PPG')\n",
    "\n",
    "# Highlight window regions\n",
    "for i, win_idx in enumerate(segment_window_indices):\n",
    "    window_num = int(window_ids[win_idx].split('_W')[-1])\n",
    "    start_sample = window_num * stride_samples\n",
    "    end_sample = start_sample + samples_per_window\n",
    "\n",
    "    start_time = start_sample / SAMPLING_RATE\n",
    "    end_time = end_sample / SAMPLING_RATE\n",
    "\n",
    "    ax2.axvspan(start_time, end_time, alpha=0.2, color=colors[i % len(colors)],\n",
    "                label=f'W{window_num}')\n",
    "\n",
    "ax2.set_xlabel('Time (seconds)', fontsize=11)\n",
    "ax2.set_ylabel('Amplitude', fontsize=11)\n",
    "ax2.set_title('Original PPG Segment (10s) with Window Regions', fontsize=12, fontweight='bold', pad=10) # Increased pad\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(loc='upper right', fontsize=9)\n",
    "ax2.set_xlim([0, 10])\n",
    "ax2.set_ylim([-1.1, 1.1])\n",
    "\n",
    "# ===== ROWS 1-4: Individual Windows (ECG left, PPG right) =====\n",
    "n_windows = len(segment_window_indices)\n",
    "\n",
    "for i, win_idx in enumerate(segment_window_indices):\n",
    "    if i >= 5:  # Limit to 5 windows\n",
    "        break\n",
    "\n",
    "    row = 1 + i\n",
    "\n",
    "    ecg_win = ecg_windows[win_idx]\n",
    "    ppg_win = ppg_windows[win_idx]\n",
    "    window_id = window_ids[win_idx]\n",
    "    window_num = int(window_id.split('_W')[-1])\n",
    "\n",
    "    # ECG Window (left column)\n",
    "    ax_ecg = fig.add_subplot(gs[row, 0])\n",
    "    ax_ecg.plot(time_window, ecg_win, color=colors[i % len(colors)], linewidth=1.5)\n",
    "    ax_ecg.set_xlabel('Time (seconds)', fontsize=10)\n",
    "    ax_ecg.set_ylabel('Amplitude', fontsize=10)\n",
    "    ax_ecg.set_title(f'ECG Window {window_num} (ID: {window_id})', fontsize=11, fontweight='bold', pad=5) # Increased pad\n",
    "    ax_ecg.grid(True, alpha=0.3)\n",
    "    ax_ecg.set_xlim([0, 4])\n",
    "    ax_ecg.set_ylim([-1.1, 1.1])\n",
    "    ax_ecg.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "    # Add range info\n",
    "    ax_ecg.text(0.02, 0.98, f'Range: [{ecg_win.min():.2f}, {ecg_win.max():.2f}]',\n",
    "               transform=ax_ecg.transAxes, fontsize=9,\n",
    "               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    # PPG Window (right column)\n",
    "    ax_ppg = fig.add_subplot(gs[row, 1])\n",
    "    ax_ppg.plot(time_window, ppg_win, color=colors[i % len(colors)], linewidth=1.5)\n",
    "    ax_ppg.set_xlabel('Time (seconds)', fontsize=10)\n",
    "    ax_ppg.set_ylabel('Amplitude', fontsize=10)\n",
    "    ax_ppg.set_title(f'PPG Window {window_num} (ID: {window_id})', fontsize=11, fontweight='bold', pad=5) # Increased pad\n",
    "    ax_ppg.grid(True, alpha=0.3)\n",
    "    ax_ppg.set_xlim([0, 4])\n",
    "    ax_ppg.set_ylim([-1.1, 1.1])\n",
    "    ax_ppg.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "    # Add range info\n",
    "    ax_ppg.text(0.02, 0.98, f'Range: [{ppg_win.min():.2f}, {ppg_win.max():.2f}]',\n",
    "               transform=ax_ppg.transAxes, fontsize=9,\n",
    "               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.savefig('segment_windowing_ecg_ppg_visualization.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Visualization saved as 'segment_windowing_ecg_ppg_visualization.png'\")\n",
    "\n",
    "# Print detailed information\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"WINDOW DETAILS (ECG & PPG)\")\n",
    "print(f\"{'='*70}\")\n",
    "for i, win_idx in enumerate(segment_window_indices):\n",
    "    window_id = window_ids[win_idx]\n",
    "    window_num = int(window_id.split('_W')[-1])\n",
    "    start_time = window_num * metadata['stride_sec']\n",
    "    end_time = start_time + metadata['window_duration_sec']\n",
    "\n",
    "    ecg_win = ecg_windows[win_idx]\n",
    "    ppg_win = ppg_windows[win_idx]\n",
    "\n",
    "    print(f\"\\nWindow {window_num} (ID: {window_id}):\")\n",
    "    print(f\"  Time range:      [{start_time:.1f}s - {end_time:.1f}s]\")\n",
    "    print(f\"  ECG shape:       {ecg_win.shape}\")\n",
    "    print(f\"  ECG range:       [{ecg_win.min():.3f}, {ecg_win.max():.3f}]\")\n",
    "    print(f\"  ECG mean/std:    {ecg_win.mean():.3f} / {ecg_win.std():.3f}\")\n",
    "    print(f\"  PPG shape:       {ppg_win.shape}\")\n",
    "    print(f\"  PPG range:       [{ppg_win.min():.3f}, {ppg_win.max():.3f}]\")\n",
    "    print(f\"  PPG mean/std:    {ppg_win.mean():.3f} / {ppg_win.std():.3f}\")\n",
    "    print(f\"  ✅ Paired:       ECG and PPG have same window ID\")\n",
    "\n",
    "# Close files\n",
    "original_batch.close()\n",
    "windowed_batch.close()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✅ Visualization complete!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX7u6vImtCuF"
   },
   "source": [
    "# use the initial Normalized data and resample to 100Hz for ECG CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JylxFw1tLk-",
    "outputId": "d79532cf-5dbc-49bd-8c16-4a55c61a72a1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_With_BMI'\n",
    "normalized_dir = '/content/drive/MyDrive/CAS_FinalProject/Vital_Normalized_Resampled_100Hz'\n",
    "os.makedirs(normalized_dir, exist_ok=True)\n",
    "\n",
    "def normalize_to_range(signal, min_val=-1, max_val=1):\n",
    "    \"\"\"\n",
    "    Normalize signal to specified range [min_val, max_val]\n",
    "    Default: [-1, 1]\n",
    "    \"\"\"\n",
    "    sig_min = signal.min()\n",
    "    sig_max = signal.max()\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if sig_max - sig_min == 0:\n",
    "        return np.zeros_like(signal)\n",
    "\n",
    "    # Min-Max normalization formula\n",
    "    normalized = (signal - sig_min) / (sig_max - sig_min)  # [0, 1]\n",
    "    normalized = normalized * (max_val - min_val) + min_val  # [min_val, max_val]\n",
    "\n",
    "    return normalized\n",
    "\n",
    "# Process each batch\n",
    "batch_files = sorted(glob.glob(os.path.join(save_dir, 'PulseDB_Vital_Batch_*.npz')))\n",
    "\n",
    "print(f\"Processing {len(batch_files)} batches...\\n\")\n",
    "\n",
    "for idx, batch_file in enumerate(batch_files):\n",
    "    batch_num = int(os.path.basename(batch_file).split('_')[-1].replace('.npz', ''))\n",
    "\n",
    "    print(f\"Processing Batch {batch_num:04d}...\", end=' ')\n",
    "\n",
    "    # Load batch\n",
    "    data = np.load(batch_file, allow_pickle=True)\n",
    "    signals = data['Signals']  # Shape: [n_segments, 3, original_length] @ 125 Hz\n",
    "    demographics = data['Demographics']\n",
    "    subject_ids = data['SubjectIDs']\n",
    "\n",
    "    # Get original shape\n",
    "    n_segments, n_channels, original_length = signals.shape\n",
    "    print(f\"Original: {n_segments} segs x {n_channels} ch x {original_length} samples\")\n",
    "\n",
    "    # Step 1: Normalize each segment individually\n",
    "    signals_normalized = np.zeros_like(signals)\n",
    "    for seg_idx in range(n_segments):\n",
    "        # Normalize ECG (channel 0)\n",
    "        signals_normalized[seg_idx, 0, :] = normalize_to_range(signals[seg_idx, 0, :])\n",
    "        # Normalize PPG (channel 1)\n",
    "        signals_normalized[seg_idx, 1, :] = normalize_to_range(signals[seg_idx, 1, :])\n",
    "        # Keep Time (channel 2) as is\n",
    "        signals_normalized[seg_idx, 2, :] = signals[seg_idx, 2, :]\n",
    "\n",
    "    # Step 2: Resample to 100 Hz for 10 seconds (1000 samples)\n",
    "    signals_resampled = np.zeros((n_segments, n_channels, 1000))  # Target: [n_seg, 3, 1000]\n",
    "\n",
    "    for seg_idx in range(n_segments):\n",
    "        for ch_idx in range(n_channels):\n",
    "            # Resample each channel independently to 1000 samples (10 sec @ 100 Hz)\n",
    "            signals_resampled[seg_idx, ch_idx, :] = resample(\n",
    "                signals_normalized[seg_idx, ch_idx, :],\n",
    "                1000,  # Target length for CPC input_size=1000\n",
    "                axis=-1\n",
    "            )\n",
    "\n",
    "    # Save resampled batch (CPC-ready!)\n",
    "    output_path = os.path.join(normalized_dir, f'PulseDB_Vital_Batch_{batch_num:04d}_Resampled100Hz.npz')\n",
    "    np.savez_compressed(output_path,\n",
    "                        Signals=signals_resampled,           # [n_seg, 3, 1000] @ 100 Hz\n",
    "                        Signals_Normalized=signals_normalized,  # Keep normalized original\n",
    "                        Signals_Original=signals,            # Keep raw original\n",
    "                        Demographics=demographics,\n",
    "                        SubjectIDs=subject_ids)\n",
    "\n",
    "    print(f\"✅ Saved ({n_segments} segments: {original_length}→1000 samples @100Hz)\")\n",
    "\n",
    "    # Clean up\n",
    "    data.close()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✅ Resampling + Normalization COMPLETE!\")\n",
    "print(f\"CPC-ready batches saved to: {normalized_dir}\")\n",
    "print(f\"Format: [segments, 3_channels, 1000_samples] @ 100 Hz (10 sec)\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
